{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f502bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "# visualization \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "device = torch.device(\"cpu\"); gpu_id = -1 # select CPU\n",
    "\n",
    "gpu_id = '0' # select a single GPU  \n",
    "#gpu_id = '2,3' # select multiple GPUs  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)  \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('GPU name: {:s}, gpu_id: {:s}'.format(torch.cuda.get_device_name(0),gpu_id))   \n",
    "    \n",
    "print(device)\n",
    "print('pytorch version = ',torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409c1fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MLP, DynamicEdgeConv, global_max_pool, GravNetConv\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,space_dimensions=4, propagate_dimensions=22 ,k=20, aggr='max'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = GravNetConv(in_channels,64,space_dimensions, propagate_dimensions, k, aggr)\n",
    "        self.conv2 = GravNetConv(64,128,space_dimensions, propagate_dimensions, k, aggr)\n",
    "        self.lin1 = Linear(128 + 64, 1024)\n",
    "\n",
    "        self.mlp = MLP([1024, 512, 256, out_channels], dropout=0.5,\n",
    "                       batch_norm=False)\n",
    "\n",
    "    def forward(self, data):\n",
    "        pos, batch, mask = data.pos, data.batch, data.mask\n",
    "        x1 = self.conv1(pos, batch)\n",
    "        x2 = self.conv2(x1, batch)\n",
    "        out = self.lin1(torch.cat([x1, x2], dim=1))\n",
    "        out = global_max_pool(out, batch)\n",
    "        out = self.mlp(out)\n",
    "        out = out.repeat_interleave(4,dim=1)\n",
    "        out = 10 * torch.tanh(out)\n",
    "        out = out + mask\n",
    "        return F.softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63658724",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This part I designed the rectangle-characterized TSP, that means for every step the agent walk through a corner,\n",
    "then he travel through the whole rectangle using zig-zag, finally he ends up at one of the rest corners of \n",
    "the rextangle, so, it equals the agent walk through three points at one step, in practice, I add three points into \n",
    "mask to make them unselectable.\n",
    "'''\n",
    "def rectangle_process(temp,idx,Y,Y0,mask,k,B,i,path_gazebo, visit_count, visit_time_count, X_gcn_original, if_actor):\n",
    "    Y1 = Y[zero_to_bsz, idx.data].clone()\n",
    "    rectangle_inf = idx/4\n",
    "    feature_table = temp.outcorner_getout(rectangle_inf,B)\n",
    "    feature_table = torch.Tensor(feature_table).type(torch.long)\n",
    "    if torch.cuda.is_available():\n",
    "        feature_table = feature_table.cuda()\n",
    "    Y_corner = Y[zero_to_bsz, feature_table[:,0].data].clone()\n",
    "    add_time = visit_time_count[zero_to_bsz, feature_table[:,0].data]*((range_of_wait/2)*speed_of_nozzle)\n",
    "    if torch.cuda.is_available():\n",
    "        add_time = add_time.cuda()\n",
    "    Y_corner[:,dimension-2] = add_time\n",
    "    X_gcn_transfer = X_gcn_original.clone()\n",
    "    with torch.no_grad():\n",
    "        for i in range(128):\n",
    "            X_gcn_transfer[i][idx.data[i]][dimension-2] = add_time[i]\n",
    "            X_gcn_transfer[i][feature_table[:,0].data[i]][dimension-2] = add_time[i]\n",
    "            X_gcn_transfer[i][feature_table[:,1].data[i]][dimension-2] = add_time[i]\n",
    "            X_gcn_transfer[i][feature_table[:,2].data[i]][dimension-2] = add_time[i]\n",
    "    if (i % 100 == 0)and(if_actor):\n",
    "        path_gazebo.append([idx.data[0].tolist(),feature_table[:,0].data[0].tolist()])\n",
    "    if k ==0:\n",
    "        if torch.cuda.is_available():\n",
    "            reward = torch.zeros(B).cuda()\n",
    "        else:\n",
    "            reward = torch.zeros(B)\n",
    "    if k > 0:\n",
    "        reward = torch.sum((Y1[:,(0,1)] - Y0[:,(0,1)])**2 , dim=1 )**0.5\n",
    "        reward += torch.sum((Y_corner[:,(0,1)] - Y1[:,(0,1)])**2 , dim=1 )**0.5\n",
    "        #dis = (Y1 - Y0)**2\n",
    "        #dis_1 = (Y_corner - Y1)**2\n",
    "        #reward = torch.maximum(dis[:,0],dis[:,1])**0.5\n",
    "        #reward += torch.maximum(dis_1[:,0],dis_1[:,1])**0.5  \n",
    "    visit_count[zero_to_bsz, idx.data] -= 1\n",
    "    visit_count[zero_to_bsz, feature_table[:,0].data] -= 1\n",
    "    visit_count[zero_to_bsz, feature_table[:,1].data] -= 1\n",
    "    visit_count[zero_to_bsz, feature_table[:,2].data] -= 1\n",
    "    visit_time_count[zero_to_bsz, idx.data] += 1\n",
    "    visit_time_count[zero_to_bsz, feature_table[:,0].data] += 1\n",
    "    visit_time_count[zero_to_bsz, feature_table[:,1].data] += 1\n",
    "    visit_time_count[zero_to_bsz, feature_table[:,2].data] += 1\n",
    "    if_revisited = visit_count[zero_to_bsz, idx.data]<=0\n",
    "    if_revisited = -np.inf*if_revisited\n",
    "    if_revisited[if_revisited!=if_revisited] = 0\n",
    "    mask[zero_to_bsz, idx.data] += if_revisited\n",
    "    mask[zero_to_bsz, feature_table[:,0].data] += if_revisited\n",
    "    mask[zero_to_bsz, feature_table[:,1].data] += if_revisited  \n",
    "    mask[zero_to_bsz, feature_table[:,2].data] += if_revisited\n",
    "    \n",
    "    \n",
    "    \n",
    "    return Y, reward, Y_corner, Y_corner,feature_table[:,0], X_gcn_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e13731",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import knn_graph\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from listofpathpoint import input_handler\n",
    "import yaml\n",
    "from torch.distributions.categorical import Categorical\n",
    "# visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from tqdm import tqdm_notebook\n",
    "import torch.nn.functional as F\n",
    "####### my own import file ##########\n",
    "from listofpathpoint import input_handler\n",
    "import cnc_input\n",
    "import copy\n",
    "#from hybrid_models import HPN\n",
    "####### my own import file ##########\n",
    "'''\n",
    "so, the models we have are TransEncoderNet,\n",
    "                            Attention\n",
    "                            LSTM\n",
    "                            HPN\n",
    "each one have initial parameters and the forward part, \n",
    "once we have the forward part, the back propagation will \n",
    "finished automatically by pytorch  \n",
    "'''\n",
    "TOL = 1e-3\n",
    "TINY = 1e-15\n",
    "learning_rate = 0.005   #learning rate\n",
    "B = 128          #batch size\n",
    "B_valLoop = 1\n",
    "steps = 2500\n",
    "n_epoch = 100       # epochs\n",
    "map_number = 0\n",
    "record_actor = []\n",
    "record_critic = []\n",
    "dimension = 4\n",
    "speed_of_nozzle = 30\n",
    "range_of_wait = 20\n",
    "scale_of_the_map = (800,500,speed_of_nozzle*range_of_wait)\n",
    "maplist = ['10&15data/25_chips/25_1.json'\n",
    "          ,'10&15data/25_chips/25_1.json','10&15data/25_chips/25_1.json'\\\n",
    "          ,'10&15data/25_chips/25_1.json','10&15data/25_chips/25_1.json'\\\n",
    "          ,'10&15data/25_chips/25_1.json'\n",
    "          ]\n",
    "print('======================')\n",
    "print('prepare to train')\n",
    "print('======================')\n",
    "print('Hyper parameters:')\n",
    "print('learning rate', learning_rate)\n",
    "print('batch size', B)\n",
    "print('steps', steps)\n",
    "print('epoch', n_epoch)\n",
    "print('======================')\n",
    "\n",
    "'''\n",
    "instantiate a training network and a baseline network\n",
    "'''\n",
    "temp = input_handler(maplist[map_number])\n",
    "x_gcn_temp, mask_list_num = temp.final_ver_points_no_visitcount()\n",
    "\n",
    "try:\n",
    "    del Actor  # remove existing model\n",
    "    del Critic # remove existing model\n",
    "except:\n",
    "    pass\n",
    "Actor = Net(in_channels=3, out_channels=(len(mask_list_num)-1)).to(device)\n",
    "Critic = Net(in_channels=3, out_channels=(len(mask_list_num)-1)).to(device)\n",
    "optimizer = optim.Adam(Actor.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "# Putting Critic model on the eval mode\n",
    "Actor = Actor.to(device)\n",
    "Critic = Critic.to(device)\n",
    "Critic.eval()\n",
    "\n",
    "epoch_ckpt = 0\n",
    "tot_time_ckpt = 0\n",
    "\n",
    "val_mean = []\n",
    "val_std = []\n",
    "          \n",
    "plot_performance_train = []\n",
    "plot_performance_baseline = []\n",
    "# recording the result of the resent epoch makes it available for future\n",
    "#*********************# Uncomment these lines to load the previous check point\n",
    "\"\"\"\n",
    "checkpoint_file = \"checkpoint/mutimap_20.pkl\"\n",
    "checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "epoch_ckpt = checkpoint['epoch'] + 1\n",
    "tot_time_ckpt = checkpoint['tot_time']\n",
    "plot_performance_train = checkpoint['plot_performance_train']\n",
    "plot_performance_baseline = checkpoint['plot_performance_baseline']\n",
    "Critic.load_state_dict(checkpoint['model_baseline'])\n",
    "Actor.load_state_dict(checkpoint['model_train'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "print('Re-start training with saved checkpoint file={:s}\\n  Checkpoint at epoch= {:d} and time={:.3f}min\\n'.format(checkpoint_file,epoch_ckpt-1,tot_time_ckpt/60))\n",
    "\"\"\"\n",
    "\n",
    "#***********************# Uncomment these lines to load the previous check point\n",
    "\n",
    "# Main training loop\n",
    "# The core training concept mainly upon Sampling from the actor\n",
    "# then taking the greedy action from the critic\n",
    "\n",
    "start_training_time = time.time()\n",
    "time_stamp = datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\") # Load the time stamp\n",
    "\n",
    "C = 0       # baseline => the object which the actor can compare\n",
    "R = 0       # reward\n",
    "# load 100 points into the program \n",
    "x_gcn_temp = torch.Tensor(x_gcn_temp).type(torch.float)\n",
    "X_temp, mask_list_num = temp.final_ver_points()\n",
    "print(X_temp)\n",
    "print(mask_list_num)\n",
    "#change the waiting time to the distance that we can measure\n",
    "size = len(X_temp)\n",
    "zero_to_bsz = torch.arange(B, device = device) # a list contains 0 to (batch size -1)\n",
    "batch = torch.arange(B)\n",
    "batch = batch.repeat_interleave(size)\n",
    "for corner in X_temp:\n",
    "    corner[dimension -2] *= speed_of_nozzle\n",
    "size_rec = mask_list_num[-1]\n",
    "print(size_rec)\n",
    "\n",
    "X_temp = torch.FloatTensor(X_temp)\n",
    "visit_count_initial = X_temp[:,dimension-1]\n",
    "visit_count_rec = []\n",
    "for i in range(0,len(visit_count_initial),4):\n",
    "    visit_count_rec.append(int(visit_count_initial[i]))\n",
    "X = X_temp.repeat(B,1,1)\n",
    "if torch.cuda.is_available():\n",
    "    X = X.cuda()\n",
    "\n",
    "for epoch in range(0, n_epoch):\n",
    "    # re-start training with saved checkpoint\n",
    "    epoch += epoch_ckpt # adding the number of the former epochs\n",
    "    # Train the model for one epoch\n",
    "    start = time.time() # record the starting time\n",
    "    Actor.train() \n",
    "    path_gazebo = []\n",
    "    for i_all in range(1, steps+1): # 1 ~ 2500 steps\n",
    "        # mask some points that are not the first visited points\n",
    "        if torch.cuda.is_available():\n",
    "            R = torch.zeros(B).cuda()\n",
    "            reward_recorder = torch.zeros(B,size_rec).cuda()\n",
    "            idx_recorder = torch.zeros(B,size_rec).cuda()\n",
    "            logprobs = 0\n",
    "            reward = torch.zeros(B).cuda()\n",
    "            mask = torch.zeros(B,size).cuda() # use mask to make some points impossible to choose\n",
    "            X_gcn_original = x_gcn_temp.repeat(B,1,1).cuda()\n",
    "        else:\n",
    "            R = torch.zeros(B)\n",
    "            reward_recorder = torch.zeros(B,size_rec)\n",
    "            idx_recorder = torch.zeros(B,size_rec)\n",
    "            logprobs = 0\n",
    "            reward = torch.zeros(B)\n",
    "            mask = torch.zeros(B,size) # use mask to make some points impossible to choose\n",
    "            X_gcn_original = x_gcn_temp.repeat(B,1,1)\n",
    "        x = torch.zeros(B,dimension)# Y[:,0,:] #set the first point to x\n",
    "        h = None\n",
    "        c = None\n",
    "        context = None        # set Y_ini to the out corner\n",
    "        Transcontext = None\n",
    "        Y0 = None\n",
    "        choosen_idx = None\n",
    "        visit_count = visit_count_initial.repeat(B,1)\n",
    "        visit_time_count_initial = torch.zeros(len(visit_count_initial))\n",
    "        visit_time_count = visit_time_count_initial.repeat(B,1)\n",
    "        if torch.cuda.is_available():\n",
    "            visit_count = visit_count.cuda()\n",
    "        # Actor Sampling phase\n",
    "        for k in range(size_rec):\n",
    "            if k ==0:\n",
    "                Y = X.view(B,size,dimension)\n",
    "            X_gcn = X_gcn_original.view(B*size,dimension-1)\n",
    "            print(k)\n",
    "            data = Data(pos=X_gcn,batch=batch,mask=mask)\n",
    "            output = Actor(data)\n",
    "            sampler = torch.distributions.Categorical(output)\n",
    "            idx = sampler.sample()\n",
    "            idx_recorder[:,k] = (idx/4).type(torch.long)\n",
    "            # prepare for the back propagation of pytorch\n",
    "            Y, reward, Y0, x, choosen_idx, X_gcn_original = rectangle_process(temp, idx,Y,Y0,mask,k,B,i_all,path_gazebo,\n",
    "                                                visit_count, visit_time_count, X_gcn_original, if_actor=True)\n",
    "\n",
    "            # change the input data X_gcn to match the current situation on map\n",
    "            R += reward\n",
    "            reward_recorder[:,k] = R\n",
    "            logprobs += torch.log(output[zero_to_bsz, idx.data] + TINY)\n",
    "            \n",
    "        # now it's time to check if any trajectory should be punished due to waiting on the same rectangle\n",
    "\n",
    "        trajec_count = 0\n",
    "        if i_all%100 ==0:\n",
    "            print(idx_recorder[0])\n",
    "        for path_time in zip(idx_recorder,reward_recorder):\n",
    "            idx_time = zip(path_time[0],path_time[1])\n",
    "            idx_time = sorted(idx_time, key=lambda x: x[0])\n",
    "            total_idx = 0\n",
    "            extra_waiting_time = 0\n",
    "            for item in visit_count_rec:\n",
    "                compare_list = []\n",
    "                if item>1:\n",
    "                    for i in range(int(item)):\n",
    "                        compare_list.append(idx_time[int(total_idx + i)][1])\n",
    "                    compare_list.sort(reverse=True)\n",
    "                    total_idx += item\n",
    "                    for i in range(len(compare_list)-1):\n",
    "                        dis_step = compare_list[i] - compare_list[i+1]\n",
    "                        dry_time = (range_of_wait/2)*speed_of_nozzle\n",
    "                        if (dis_step) < (dry_time):\n",
    "                            extra_waiting_time += dry_time - dis_step\n",
    "                else:\n",
    "                    total_idx += item\n",
    "            R[trajec_count] += extra_waiting_time\n",
    "            trajec_count+=1\n",
    "                    \n",
    "                \n",
    "        \n",
    "# # critic baseline phase, use the baseline to compute the actual reward of agent at that time\n",
    "        if torch.cuda.is_available():\n",
    "            C = torch.zeros(B).cuda()\n",
    "            reward_recorder = torch.zeros(B,size_rec).cuda()\n",
    "            idx_recorder = torch.zeros(B,size_rec).cuda()\n",
    "            baseline = torch.zeros(B).cuda()\n",
    "            mask = torch.zeros(B,size).cuda() # use mask to make some points impossible to choose\n",
    "            X_gcn_original = x_gcn_temp.repeat(B,1,1).cuda()\n",
    "        else:\n",
    "            C = torch.zeros(B)\n",
    "            reward_recorder = torch.zeros(B,size_rec)\n",
    "            idx_recorder = torch.zeros(B,size_rec)\n",
    "            baseline = torch.zeros(B)\n",
    "            mask = torch.zeros(B,size) # use mask to make some points impossible to choose\n",
    "            X_gcn_original = x_gcn_temp.repeat(B,1,1)\n",
    "        x = torch.zeros(B,dimension)#Y[:,0,:] #set the first point to x\n",
    "        h = None\n",
    "        c = None\n",
    "        context = None\n",
    "        Transcontext = None\n",
    "        C0 = None\n",
    "        choosen_idx = None\n",
    "        visit_count = visit_count_initial.repeat(B,1)\n",
    "        visit_time_count_initial = torch.zeros(len(visit_count_initial))\n",
    "        visit_time_count = visit_time_count_initial.repeat(B,1)\n",
    "        if torch.cuda.is_available():\n",
    "            visit_count = visit_count.cuda()\n",
    "        # compute tours for baseline without grad \"Cause we want to fix the weights for the critic\"\n",
    "        with torch.no_grad():\n",
    "            for k in range(size_rec): \n",
    "                if k ==0:\n",
    "                    Y = X.view(B,size,dimension)\n",
    "                #check if all the points to be masked,if so, raise the total reward R, and check again\n",
    "                X_gcn = X_gcn_original.view(B*size,dimension-1)\n",
    "                data = Data(pos=X_gcn,batch=batch,mask=mask)\n",
    "                output = Critic(data)\n",
    "                idx = torch.argmax(output, dim=1) # ----> greedy baseline critic\n",
    "                idx_recorder[:,k] = (idx/4).type(torch.long)\n",
    "                # prepare for the back propagation of pytorch\n",
    "                Y, baseline, C0, x, choosen_idx, X_gcn_original = rectangle_process(temp,idx,Y,C0, mask,k,B,i,path_gazebo,\n",
    "                                                   visit_count, visit_time_count, X_gcn_original, if_actor=False)\n",
    "                C += baseline\n",
    "                reward_recorder[:,k] = C\n",
    "                    #now is the time to check if any trajectory should be punished due to waiting on the same rectangle\n",
    "\n",
    "            trajec_count = 0\n",
    "            for path_time in zip(idx_recorder,reward_recorder):\n",
    "                idx_time = zip(path_time[0],path_time[1])\n",
    "                idx_time = sorted(idx_time, key=lambda x: x[0])\n",
    "                total_idx = 0\n",
    "                extra_waiting_time = 0\n",
    "                for item in visit_count_rec:\n",
    "                    compare_list = []\n",
    "                    if item>1:\n",
    "                        for i in range(int(item)):\n",
    "                            compare_list.append(idx_time[int(total_idx + i)][1])\n",
    "                        compare_list.sort(reverse=True)\n",
    "                        total_idx += item\n",
    "                        for i in range(len(compare_list)-1):\n",
    "                            dis_step = compare_list[i] - compare_list[i+1]\n",
    "                            dry_time = (range_of_wait/2)*speed_of_nozzle\n",
    "                            if (dis_step) < (dry_time):\n",
    "                                extra_waiting_time += dry_time - dis_step\n",
    "                    else:\n",
    "                        total_idx += item\n",
    "                C[trajec_count] += extra_waiting_time\n",
    "                trajec_count+=1\n",
    "        ###################\n",
    "        # Loss and backprop handling \n",
    "        ###################\n",
    "        loss = torch.mean((R - C) * logprobs)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i_all % 100 == 0:\n",
    "            print(\"epoch:{}, batch:{}/{}, reward:{}\".format(epoch, i_all, steps,R.mean().item()))\n",
    "            record_actor.append(R.mean().tolist())\n",
    "            record_critic.append(C.mean().tolist())\n",
    "            plt.plot(record_actor,'r:')\n",
    "            plt.plot(record_critic,'b:')\n",
    "            plt.show()\n",
    "        if i_all % 100 == 0:\n",
    "            print(\"record the last path to gazebo for showing up\")\n",
    "            #starting to show the path on simulated enviroment of cnc_machine \n",
    "            the_resent_path = temp.zig_zag_path(path_gazebo,mask_list_num)\n",
    "            data = {'path':the_resent_path}\n",
    "            data_1 = {'corners':path_gazebo}\n",
    "            pathpoints_dir = os.path.join(\"pathpoints\")\n",
    "            if not os.path.exists(pathpoints_dir):\n",
    "                os.makedirs(pathpoints_dir)\n",
    "            name = 'pathpoints/path_points '+str(i)+'.yaml'\n",
    "            with open(name, 'w') as file:\n",
    "                documents = yaml.dump(data,file)\n",
    "                documents = yaml.dump(data_1,file)\n",
    "            path_gazebo = []\n",
    "    time_one_epoch = time.time() - start #recording the work time of one epoch\n",
    "    time_tot = time.time() - start_training_time + tot_time_ckpt\n",
    "    ###################\n",
    "    # Evaluate train model and baseline \n",
    "    # in this phase we just solve random instances with the actor and the critic\n",
    "    # compare this soluation if we get any improvment we'll transfer the actor's\n",
    "    # weights into the critic\n",
    "    ###################\n",
    "    # putting the actor in the eval mode\n",
    "    Actor.eval()\n",
    "    \n",
    "    mean_tour_length_actor = 0\n",
    "    mean_tour_length_critic = 0\n",
    "\n",
    "    for step in range(0,B_valLoop):\n",
    "        \n",
    "        # compute tour for model and baseline\n",
    "        # mask some points that are not the first visited points\n",
    "        if torch.cuda.is_available():\n",
    "            R = torch.zeros(B).cuda()\n",
    "            reward_recorder = torch.zeros(B,size_rec).cuda()\n",
    "            idx_recorder = torch.zeros(B,size_rec).cuda()\n",
    "            reward = torch.zeros(B).cuda()\n",
    "            mask = torch.zeros(B,size).cuda() # use mask to make some points impossible to choose\n",
    "            X_gcn_original = x_gcn_temp.repeat(B,1,1).cuda()\n",
    "        else:\n",
    "            R = torch.zeros(B)\n",
    "            reward_recorder = torch.zeros(B,size_rec)\n",
    "            idx_recorder = torch.zeros(B,size_rec)\n",
    "            reward = torch.zeros(B)\n",
    "            mask = torch.zeros(B,size) # use mask to make some points impossible to choose\n",
    "            X_gcn_original = x_gcn_temp.repeat(B,1,1)\n",
    "        x = torch.zeros(B,dimension)# Y[:,0,:] #set the first point to x\n",
    "        h = None\n",
    "        c = None\n",
    "        context = None        # set Y_ini to the out corner\n",
    "        Transcontext = None\n",
    "        Y0 = None\n",
    "        choosen_idx = None\n",
    "        visit_count = visit_count_initial.repeat(B,1)\n",
    "        visit_time_count_initial = torch.zeros(len(visit_count_initial))\n",
    "        visit_time_count = visit_time_count_initial.repeat(B,1)\n",
    "        if torch.cuda.is_available():\n",
    "            visit_count = visit_count.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for k in range(size_rec):\n",
    "                if k==0:\n",
    "                    Y = X.view(B,size,dimension)\n",
    "                X_gcn = X_gcn_original.view(B*size,dimension-1)\n",
    "                data = Data(pos=X_gcn,batch=batch,mask=mask)\n",
    "                output = Actor(data)\n",
    "                idx = torch.argmax(output, dim=1)\n",
    "                idx_recorder[:,k] = (idx/4).type(torch.long)\n",
    "                #prepare for the back propagation of pytorch\n",
    "                Y, reward, Y0, x, choosen_idx, X_gcn_original = rectangle_process(temp, idx,Y,Y0,mask,k,B,i,path_gazebo,\n",
    "                                                    visit_count, visit_time_count, X_gcn_original, if_actor=False)\n",
    "                R += reward\n",
    "                reward_recorder[:,k] = R\n",
    "            #now is the time to check if any trajectory should be punished due to waiting on the same rectangle\n",
    "\n",
    "            trajec_count = 0\n",
    "            for path_time in zip(idx_recorder,reward_recorder):\n",
    "                idx_time = zip(path_time[0],path_time[1])\n",
    "                idx_time = sorted(idx_time, key=lambda x: x[0])\n",
    "                total_idx = 0\n",
    "                extra_waiting_time = 0\n",
    "                for item in visit_count_rec:\n",
    "                    compare_list = []\n",
    "                    if item>1:\n",
    "                        for i in range(int(item)):\n",
    "                            compare_list.append(idx_time[int(total_idx + i)][1])\n",
    "                        compare_list.sort(reverse=True)\n",
    "                        total_idx += item\n",
    "                        for i in range(len(compare_list)-1):\n",
    "                            dis_step = compare_list[i] - compare_list[i+1]\n",
    "                            dry_time = (range_of_wait/2)*speed_of_nozzle\n",
    "                            if (dis_step) < (dry_time):\n",
    "                                extra_waiting_time += dry_time - dis_step\n",
    "                    else:\n",
    "                        total_idx += item\n",
    "                R[trajec_count] += extra_waiting_time\n",
    "                trajec_count+=1\n",
    "            print('R_val = ',R[0])\n",
    "        if torch.cuda.is_available():\n",
    "            C = torch.zeros(B).cuda()\n",
    "            reward_recorder = torch.zeros(B,size_rec).cuda()\n",
    "            idx_recorder = torch.zeros(B,size_rec).cuda()\n",
    "            baseline = torch.zeros(B).cuda()\n",
    "            mask = torch.zeros(B,size).cuda() # use mask to make some points impossible to choose\n",
    "            X_gcn_original = x_gcn_temp.repeat(B,1,1).cuda()\n",
    "        else:\n",
    "            C = torch.zeros(B)\n",
    "            reward_recorder = torch.zeros(B,size_rec)\n",
    "            idx_recorder = torch.zeros(B,size_rec)\n",
    "            baseline = torch.zeros(B)\n",
    "            mask = torch.zeros(B,size) # use mask to make some points impossible to choose\n",
    "            X_gcn_original = x_gcn_temp.repeat(B,1,1)\n",
    "        x = torch.zeros(B,dimension)#Y[:,0,:] #set the first point to x\n",
    "        h = None\n",
    "        c = None\n",
    "        context = None\n",
    "        Transcontext = None\n",
    "        C0 = None\n",
    "        choosen_idx = None\n",
    "        visit_count = visit_count_initial.repeat(B,1)\n",
    "        visit_time_count_initial = torch.zeros(len(visit_count_initial))\n",
    "        visit_time_count = visit_time_count_initial.repeat(B,1)\n",
    "        if torch.cuda.is_available():\n",
    "            visit_count = visit_count.cuda()\n",
    "        # compute tours for baseline without grad \"Cause we want to fix the weights for the critic\"\n",
    "        with torch.no_grad():\n",
    "            for k in range(size_rec): \n",
    "                if k ==0:\n",
    "                    Y = X.view(B,size,dimension)\n",
    "                #check if all the points to be masked,if so, raise the total reward R, and check again\n",
    "                X_gcn = X_gcn_original.view(B*size,dimension-1)\n",
    "                data = Data(pos=X_gcn,batch=batch,mask=mask)\n",
    "                output = Critic(data)\n",
    "                idx = torch.argmax(output, dim=1) # ----> greedy baseline critic\n",
    "                idx_recorder[:,k] = (idx/4).type(torch.long)\n",
    "                # prepare for the back propagation of pytorch\n",
    "                Y, baseline, C0, x, choosen_idx, X_gcn_original = rectangle_process(temp,idx,Y,C0, mask,k,B,i,path_gazebo,\n",
    "                                                   visit_count, visit_time_count, X_gcn_original, if_actor=False)\n",
    "                C += baseline\n",
    "                reward_recorder[:,k] = C\n",
    "                #now is the time to check if any trajectory should be punished due to waiting on the same rectangle\n",
    "\n",
    "            trajec_count = 0\n",
    "            for path_time in zip(idx_recorder,reward_recorder):\n",
    "                idx_time = zip(path_time[0],path_time[1])\n",
    "                idx_time = sorted(idx_time, key=lambda x: x[0])\n",
    "                total_idx = 0\n",
    "                extra_waiting_time = 0\n",
    "                for item in visit_count_rec:\n",
    "                    compare_list = []\n",
    "                    if item>1:\n",
    "                        for i in range(int(item)):\n",
    "                            compare_list.append(idx_time[int(total_idx + i)][1])\n",
    "                        compare_list.sort(reverse=True)\n",
    "                        total_idx += item\n",
    "                        for i in range(len(compare_list)-1):\n",
    "                            dis_step = compare_list[i] - compare_list[i+1]\n",
    "                            dry_time = (range_of_wait/2)*speed_of_nozzle\n",
    "                            if (dis_step) < (dry_time):\n",
    "                                extra_waiting_time += dry_time - dis_step\n",
    "                    else:\n",
    "                        total_idx += item\n",
    "                C[trajec_count] += extra_waiting_time\n",
    "                trajec_count+=1\n",
    "            print('C_val = ',C.mean().item())\n",
    "        mean_tour_length_actor  += R.mean().item()\n",
    "        mean_tour_length_critic += C.mean().item()\n",
    "\n",
    "    mean_tour_length_actor  =  mean_tour_length_actor  / B_valLoop\n",
    "    mean_tour_length_critic =  mean_tour_length_critic / B_valLoop\n",
    "    # evaluate train model and baseline and update if train model is better\n",
    "\n",
    "    update_baseline = mean_tour_length_actor + TOL < mean_tour_length_critic\n",
    "\n",
    "    print('Avg Actor {} --- Avg Critic {}'.format(mean_tour_length_actor,mean_tour_length_critic))\n",
    "\n",
    "    if update_baseline:\n",
    "        Critic.load_state_dict(Actor.state_dict())\n",
    "        print('My actor is going on the right road Hallelujah :) Updated')\n",
    "    ###################\n",
    "    # Valdiation train model and baseline on 1k random TSP instances\n",
    "    ###################\n",
    "    # erased by daniel due to the 1K tsp is not the scale I want to train  \n",
    "\n",
    "    # For checkpoint\n",
    "    plot_performance_train.append([(epoch+1), mean_tour_length_actor])\n",
    "    plot_performance_baseline.append([(epoch+1), mean_tour_length_critic])\n",
    "    # compute the optimally gap ==> this is interesting because there is no LKH or other optimal algorithms \n",
    "    # for the problem like this rectangle characterized map\n",
    "    mystring_min = 'Epoch: {:d}, epoch time: {:.3f}min, tot time: {:.3f}day, L_actor: {:.3f}, L_critic: {:.3f}, update: {}'.format(\n",
    "        epoch, time_one_epoch/60, time_tot/86400, mean_tour_length_actor, mean_tour_length_critic, update_baseline)\n",
    "\n",
    "    print(mystring_min)\n",
    "    print('Save Checkpoints')\n",
    "\n",
    "    # Saving checkpoint\n",
    "    checkpoint_dir = os.path.join(\"checkpoint\")\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    \n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'time': time_one_epoch,\n",
    "        'tot_time': time_tot,\n",
    "        'loss': loss.item(),\n",
    "        'plot_performance_train': plot_performance_train,\n",
    "        'plot_performance_baseline': plot_performance_baseline,\n",
    "        'model_baseline': Critic.state_dict(),\n",
    "        'model_train': Actor.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        },'{}.pkl'.format(checkpoint_dir + \"/checkpoint_\" + time_stamp + \"-n{}\".format(size) + \"-gpu{}\".format(gpu_id)))\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "            \n",
    "                \n",
    "        \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da584a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch_geometric.nn\n",
    "from torch_geometric.nn import MLP\n",
    "from torch_geometric.data import Data\n",
    "print('pytorch version = ',torch.__version__)\n",
    "\n",
    "\n",
    "pos = torch.tensor([[2,3],[4,5],[6,5]], dtype=torch.long)\n",
    "edge_index = torch.tensor([[100, 100],\n",
    "                           [150, 870],\n",
    "                           [130, 200],], dtype=torch.long)\n",
    "x = torch.tensor([[-1,4], [0,3], [1,6], [3,4], [2,44]], dtype=torch.float)\n",
    "\n",
    "data = Data(pos=pos, x=x, edge_index=edge_index.t().contiguous())\n",
    "mlp = MLP(in_channels=16, hidden_channels=32,\n",
    "          out_channels=128, num_layers=3)\n",
    "print(data.keys)\n",
    "print(data.has_self_loops())\n",
    "print(data.has_isolated_nodes())\n",
    "print(data.num_features)\n",
    "print(data.num_nodes)\n",
    "print(data.num_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7992511c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from torch_geometric.utils.convert import to_networkx, from_networkx\n",
    "import torch\n",
    "\n",
    "\n",
    "# Make the networkx graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add some cars (just do 4 for now)\n",
    "G.add_nodes_from([\n",
    "      (683,152,0,3),\n",
    "      (623,132,0,3),\n",
    "      (456,142,0,1),\n",
    "      (432,192,0,2),\n",
    "])\n",
    "\n",
    "# Add some edges\n",
    "G.add_edges_from([\n",
    "                  (1, 2), (1, 4), (1, 5),\n",
    "                  (2, 3), (2, 4),\n",
    "                  (3, 2), (3, 5), \n",
    "                  (4, 1), (4, 2),\n",
    "                  (5, 1), (5, 3)\n",
    "])\n",
    "\n",
    "# Convert the graph into PyTorch geometric\n",
    "pyg_graph = from_networkx(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f703da",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "from torch_geometric.nn import TopKPooling\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch_geometric.nn import GravNetConv\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = GravNetConv(-1, 128)\n",
    "        self.pool1 = TopKPooling(128, ratio=0.8)\n",
    "        self.conv2 = GravNetConv(128, 128)\n",
    "        self.pool2 = TopKPooling(128, ratio=0.8)\n",
    "        self.conv3 = GravNetConv(128, 128)\n",
    "        self.pool3 = TopKPooling(128, ratio=0.8)\n",
    "        self.lin1 = torch.nn.Linear(128, 128)\n",
    "        self.lin2 = torch.nn.Linear(128, 64)\n",
    "        self.lin3 = torch.nn.Linear(64, 1)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(128)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(64)\n",
    "        self.act1 = torch.nn.ReLU()\n",
    "        self.act2 = torch.nn.ReLU()        \n",
    "  \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        print(x.dtype)\n",
    "        print(edge_index.dtype)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        print(x.shape)\n",
    "        x = self.lin1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.act2(x)      \n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = torch.sigmoid(self.lin3(x)).squeeze(1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396d8771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import knn_graph\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "def train():\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    print(output)\n",
    "    label = data.y.to(device)\n",
    "    loss = crit(output, label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "x = torch.Tensor([[-1, -1], [-1, 1], [1, -1], [1, 1]])\n",
    "dataset = x.repeat(128,1)\n",
    "x = x.repeat(128,1,1)\n",
    "loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "edge_index = torch.tensor([])\n",
    "for x_seed in x:\n",
    "    edge_index_temp = knn_graph(x_seed, k=1, loop=False)\n",
    "    edge_index_temp = torch.unsqueeze(edge_index_temp, 0)\n",
    "    edge_index = torch.cat((edge_index, edge_index_temp),0)\n",
    "x = torch.Tensor(x).type(torch.float)\n",
    "edge_index = torch.Tensor(edge_index).type(torch.long)\n",
    "data = Data(x=x[0],edge_index=edge_index[0],batch=128)\n",
    "# device = torch.device('cuda')\n",
    "model = Net()#.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "crit = torch.nn.BCELoss()\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c1e1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import ModelNet\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MLP, DynamicEdgeConv, global_max_pool, GravNetConv\n",
    "\n",
    "path = osp.join(osp.dirname(osp.realpath('only_for_test')), '..', 'data/ModelNet10')\n",
    "pre_transform, transform = T.NormalizeScale(), T.SamplePoints(1024)\n",
    "train_dataset = ModelNet(path, '10', True, transform, pre_transform)\n",
    "test_dataset = ModelNet(path, '10', False, transform, pre_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
    "                          num_workers=6)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False,\n",
    "                         num_workers=6)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, out_channels, k=20, aggr='max'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = DynamicEdgeConv(MLP([2 * 3, 64, 64, 64]), k, aggr)\n",
    "        self.conv2 = DynamicEdgeConv(MLP([2 * 64, 128]), k, aggr)\n",
    "        self.lin1 = Linear(128 + 64, 1024)\n",
    "\n",
    "        self.mlp = MLP([1024, 512, 256, out_channels], dropout=0.5,\n",
    "                       batch_norm=False)\n",
    "\n",
    "    def forward(self, data):\n",
    "        pos, batch = data.pos, data.batch\n",
    "        print(pos.shape)\n",
    "        x1 = self.conv1(pos, batch)\n",
    "        print(x1.shape)\n",
    "        x2 = self.conv2(x1, batch)\n",
    "        print(x2.shape)\n",
    "        out = self.lin1(torch.cat([x1, x2], dim=1))\n",
    "        out = global_max_pool(out, batch)\n",
    "        out = self.mlp(out)\n",
    "        return F.log_softmax(out, dim=1)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(train_dataset.num_classes, k=20).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    print(train_loader)\n",
    "    for data in train_loader:\n",
    "        print(data)\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(data).max(dim=1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch {epoch:03d}, Loss: {loss:.4f}, Test: {test_acc:.4f}')\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15da0af0",
   "metadata": {},
   "source": [
    "# 點雲圖處理模型的借鏡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f7516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MLP, DynamicEdgeConv, global_max_pool, GravNetConv\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,space_dimensions=4, propagate_dimensions=22 ,k=20, aggr='max'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = GravNetConv(in_channels,64,space_dimensions, propagate_dimensions, k, aggr)\n",
    "        self.conv2 = GravNetConv(64,128,space_dimensions, propagate_dimensions, k, aggr)\n",
    "        self.lin1 = Linear(128 + 64, 1024)\n",
    "\n",
    "        self.mlp = MLP([1024, 512, 256, out_channels], dropout=0.5,\n",
    "                       batch_norm=False)\n",
    "\n",
    "    def forward(self, data):\n",
    "        pos, batch = data.pos, data.batch\n",
    "        x1 = self.conv1(pos, batch)\n",
    "        x2 = self.conv2(x1, batch)\n",
    "        out = self.lin1(torch.cat([x1, x2], dim=1))\n",
    "        out = global_max_pool(out, batch)\n",
    "        out = self.mlp(out)\n",
    "        out = out.repeat_interleave(4,dim=1)\n",
    "        out = 10 * torch.tanh(out)\n",
    "        return F.softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a64316",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(in_channels=3, out_channels=(len(mask_list_num)-1)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec69bcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import knn_graph\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from listofpathpoint import input_handler\n",
    "batch_size = 128\n",
    "temp = input_handler('10&15data/25_chips/25_1.json')\n",
    "x, mask_list_num = temp.final_ver_points_no_visitcount()\n",
    "# load 100 points into the program \n",
    "x = torch.Tensor(x).type(torch.float)\n",
    "X_original = x.repeat(128,1,1)\n",
    "print(X_original.shape)\n",
    "batch = torch.arange(batch_size)\n",
    "batch = batch.repeat_interleave(len(x))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "optimizer.zero_grad()\n",
    "size_rec = mask_list_num[-1]\n",
    "zero_to_bsz = torch.arange(128, device = device)\n",
    "add_time = torch.ones(128)\n",
    "# get the output probability of all the point, start to calculate the cost and transfer the input data\n",
    "for k in range(size_rec):\n",
    "    print(k)\n",
    "    X = X_original.view(12800,3)\n",
    "    data = Data(pos=X,batch=batch,X_original=X_original,idx=idx)\n",
    "    data = data.to(device)\n",
    "    model.train()\n",
    "    output= model(data)\n",
    "    sampler = torch.distributions.Categorical(output)\n",
    "    idx = sampler.sample()\n",
    "    for i in range(128):\n",
    "        X_original[i][idx.data[i]][2] += 1\n",
    "    print(X[:128])\n",
    "    # prepare for the back propagation of pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428d7c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import knn_graph\n",
    "\n",
    "x = torch.Tensor([[-1, -1], [-1, 1], [1, -1], [1, 1]])\n",
    "batch = torch.tensor([0, 0, 0, 0])\n",
    "edge_index = knn_graph(x, k=2, batch=batch, loop=False)\n",
    "print(edge_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
